---
title: "Class 7: Machine Learning 1"
format: pdf
author: Paul Brencick (A17668863)
toc: True
---

## Background
Today we are going to begin our exploration of some important machine learning methods, namely **clustering** and **dimensionality reduction**. 

We are going to make up some input data for clustering where we know what the natural "clusters" are.  

The function `rnorm()` can be helpful here. 

```{r}
hist(rnorm(3000))
```

>Q. Generate 30 random numbers centered around +3 and another 30 centered at -3.

```{r}
temp <- c(rnorm(30,3),
         rnorm(30,-3) )

x <- cbind(temp, rev(temp))
plot(x)
```

## K-means Clustering

The main function in "base R" for K-means clustering is called `kmeans()`.

```{r}
km <- kmeans(x,2)
km
```

> Q. What component of the result object details the cluster sizes?

```{r}
km$size
```
> Q. What component of the results object details the cluster centers?

```{r}
km$centers
```
> Q. What component of the results object details the cluster membership vector (ie. our main result of which points lie in which cluster)?

```{r}
km$cluster
```

> Q. Plot our clustering results with points colored by cluster and also add the cluster center as a new points colored blue?

```{r}

plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15)
```
> Q. Run `kmeans()` again and this time produce 4 clusters (and call your results object `k4`) and make a result figure like above?

```{r}
k4 <- kmeans(x,4)
```

```{r}
plot(x, col=k4$cluster)
points(k4$centers, col="blue", pch=15)
```
The metric
```{r}
km$tot.withinss
k4$tot.withinss
```

> Q. Let's try different number of k (centers) from 1 to 30 and see what the best result is?

```{r}
i <-  1
ans <- NULL
for(i in 1:30) {
ans <- c(ans, kmeans(x, centers=i)$tot.withinss)
}
```

```{r}
plot(ans, typ="o")
```

**N.B** When you ask for excessive amounts of clusters, it will give you an output however it is fairly obvious on how many cluster is optimal.

## Hierarchial Clustering

The main function for hierarchical clustering is called `hclust()`. Unlike `kmeans()` (which does all the work for you) you can't just pass `hclust()` our raw input data. It needs a "distance matrix" like the one returned from the `dist()` function.

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

To extract our clusters membership vector from a `hclust()` result object we have to "cut" our tree at a given height to yield separate "groups" or "branches".

```{r}
plot(hc)
abline(h=8,col="red",lty=2)
```

To do this we use the `cuttree()` function on our `hclust()`. 

```{r}
groups <- cutree(hc,h=8)
groups
```
```{r}
table(groups,km$cluster)
```

## PCA of UK food data

Import the dataset of food consumption in the UK

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

One solution to set the row names is to do it by hand...

```{r}
rownames(x) <- x[,1]
```

To remove the first column I can use the minus index method
```{r}
x <- x[,-1]
```

A better way to do this would be to set the row name for the first column by arguing with `read.csv()`

```{r}
x <- read.csv(url,row.names=1)
x
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer to set the row name by messing with the read.csv as it is a much more future proof method of completing the task. It is also one less step to do. 

## Spotting major differences and trends

It is difficult even in this 17D dataset....

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))

```
## Pairs plots and heatmaps

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

```{r}
library(pheatmap)

pheatmap( as.matrix(x) )
```

## PCA to the rescue!

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (ie. the foods as columns and the countries as rows).

```{r}
pca <- prcomp(t(x))
```

```{r}
summary(pca)
```

To make one of the main PCA result figures, we turn to `pca$x` the scores along our new PCS. This is called "PC plot" or "Score Plot" or "Ordination Plot"...

```{r}
my_cols <- c("orange","red","blue","darkgreen")
```

```{r}
library(ggplot2)

ggplot(pca$x) + 
  aes(PC1, PC2) +
  geom_point(col=my_cols)
```

The second major result figure is called a "loading plot" of "variable contributions plot" or "weight plot"

```{r}
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))

```

